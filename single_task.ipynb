{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/ankur-98/BERT_GLUE/blob/main/single_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# For colab run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Switch to GPU runtime\r\n",
        "! git clone https://github.com/ankur-98/BERT_GLUE.git\r\n",
        "import os \r\n",
        "os.chdir(\"BERT_GLUE\")\r\n",
        "! pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\r\n",
        "from tqdm.auto import tqdm\r\n",
        "from dataloader import get_dataloader\r\n",
        "from transformers import BertForSequenceClassification\r\n",
        "from train import training_step\r\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Configs\r\n",
        "### Tasks: {\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "model_checkpoint=\"bert-base-uncased\"\r\n",
        "task = \"sst2\"\r\n",
        "batch_size=96\r\n",
        "steps = 2000\r\n",
        "lr = 2e-5\r\n",
        "lr_scheduler_type = \"linear\" # \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Dataloader and Pre-trained BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset glue (C:\\Users\\ankur\\.cache\\huggingface\\datasets\\glue\\sst2\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 68/68 [00:02<00:00, 23.35ba/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 24.39ba/s]\n",
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 23.53ba/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
            "        num_rows: 67349\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
            "        num_rows: 872\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
            "        num_rows: 1821\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
        "train_epoch_iterator = get_dataloader(task, model_checkpoint, \"train\", batch_size=batch_size)\n",
        "model = BertForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizer and LR Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "Optimizer = create_optimizer(model, learning_rate=lr)\r\n",
        "LR_scheduler = create_scheduler(Optimizer, lr_scheduler_type)\r\n",
        "Metric, Metric_1 = get_metrics(task)\r\n",
        "tr_loss = []\r\n",
        "tr_metric = []\r\n",
        "tr_metric_1 = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [27:09<00:00,  1.39it/s, loss=0.635, Accuracy=0.658]"
          ]
        }
      ],
      "source": [
        "global_steps = 0\n",
        "trange = range(len(train_epoch_iterator))\n",
        "pbar = tqdm(trange, initial=global_steps, total=steps)\n",
        "for e in range((steps//len(train_epoch_iterator))+1):\n",
        "    iterator = iter(train_epoch_iterator)\n",
        "    for step in trange:\n",
        "        global_steps += 1\n",
        "        pbar.update()\n",
        "        \n",
        "        inputs = prepare_inputs(iterator.next(), device)\n",
        "        step_loss, step_metric, step_metric_1 = training_step(model, inputs, Optimizer, LR_scheduler, Metric, Metric_1)\n",
        "        tr_loss.append(step_loss)\n",
        "        tr_metric.append(torch.tensor(list(step_metric.values())[0]))\n",
        "        if Metric_1 is not None: tr_metric_1.append(torch.tensor(list(step_metric_1.values())[0]))\n",
        "        \n",
        "        step_evaluation = {}\n",
        "        step_evaluation['loss'] = torch.stack(tr_loss[-len(train_epoch_iterator):]).mean().item()\n",
        "        step_evaluation[f\"{Metric.__class__.__name__}\"] = torch.stack(tr_metric)[-len(train_epoch_iterator):].mean().item()\n",
        "        if Metric_1 is not None:\n",
        "            step_evaluation[f\"{Metric_1.__class__.__name__}\"] = torch.stack(tr_metric_1)[-len(train_epoch_iterator):].mean().item()\n",
        "        pbar.set_postfix(step_evaluation)\n",
        "        \n",
        "        if global_steps == steps:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python385jvsc74a57bd02b5ee00ebf52c7aec0afa271013d4d845b82d463a162845fb07d4f17301b3646"
    },
    "language_info": {
      "name": "python",
      "version": ""
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}