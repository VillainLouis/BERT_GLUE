{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd02b5ee00ebf52c7aec0afa271013d4d845b82d463a162845fb07d4f17301b3646",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    from dataloader import get_dataloader\n",
    "    from transformers import BertForSequenceClassification\n",
    "    from train import training_step\n",
    "    from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "### Tasks: {\"cola\",\"mnli\",\"mnli-mm\",\"mrpc\",\"qnli\",\"qqp\",\"rte\",\"sst2\",\"stsb\",\"wnli\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_checkpoint=\"bert-base-uncased\"\n",
    "task = \"cola\"\n",
    "batch_size=96\n",
    "steps = 2000\n",
    "lr = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataloader and Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset glue (C:\\Users\\ankur\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 9/9 [00:00<00:00, 22.90ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 57.14ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 58.82ba/s]\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "train_epoch_iterator = get_dataloader(task, model_checkpoint, \"train\", batch_size=batch_size)\n",
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and LR Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Couldn't find file locally at matthews_correlation\\matthews_correlation.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.6.2/metrics/matthews_correlation/matthews_correlation.py.\nThe file was picked from the master branch on github instead at https://raw.githubusercontent.com/huggingface/datasets/master/metrics/matthews_correlation/matthews_correlation.py.\n"
     ]
    }
   ],
   "source": [
    "Optimizer = create_optimizer(model, learning_rate=lr)\n",
    "LR_scheduler = create_scheduler(Optimizer)\n",
    "Metric, Metric_1 = get_metrics(task)\n",
    "tr_loss = []\n",
    "tr_metric = []\n",
    "tr_metric_1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 200/200 [01:47<00:00,  1.51it/s, loss=0.605, MatthewsCorelation=-.000974]"
     ]
    }
   ],
   "source": [
    "global_steps = 0\n",
    "trange = range(len(train_epoch_iterator))\n",
    "pbar = tqdm(trange, initial=global_steps, total=steps)\n",
    "for e in range((steps//len(train_epoch_iterator))+1):\n",
    "    iterator = iter(train_epoch_iterator)\n",
    "    for step in trange:\n",
    "        global_steps += 1\n",
    "        pbar.update()\n",
    "        \n",
    "        inputs = prepare_inputs(iterator.next(), device)\n",
    "        step_loss, step_metric, step_metric_1 = training_step(model, inputs, Optimizer, LR_scheduler, Metric, Metric_1)\n",
    "        tr_loss.append(step_loss)\n",
    "        tr_metric.append(torch.tensor(list(step_metric.values())[0]))\n",
    "        if Metric_1 is not None: tr_metric_1.append(torch.tensor(list(step_metric_1.values())[0]))\n",
    "        \n",
    "        step_evaluation = {}\n",
    "        step_evaluation['loss'] = torch.stack(tr_loss[-len(train_epoch_iterator):]).mean().item()\n",
    "        step_evaluation[f\"{Metric.__class__.__name__}\"] = torch.stack(tr_metric)[-len(train_epoch_iterator):].mean().item()\n",
    "        if Metric_1 is not None:\n",
    "            step_evaluation[f\"{Metric_1.__class__.__name__}\"] = torch.stack(tr_metric_1)[-len(train_epoch_iterator):].mean().item()\n",
    "        pbar.set_postfix(step_evaluation)\n",
    "        \n",
    "        if global_steps == steps:\n",
    "            break"
   ]
  }
 ]
}